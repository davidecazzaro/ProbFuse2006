# ProbFuse2006

This is the project for the Information Retrieval course at Unipd.
The work is split in two parts:
  - Reimplement the basic "comb" techniques following Fox and Shaw  "Combination of Multiple Searches" paper. We also normalize the scores following Lee "Combining Multiple Evidence from Different Properties of Weighting Schemes" (1995).
  - Implement an advanced fusion ranking technique, we choose the ProbFuse2006 paper, and study the reproducibility of the paper.

# How to use
  - Use Python3
  - Create a folder in input/ called _ten_models_. Inside this folder we expect the ten run generated with Terrier of ten different IRS. So in the folder ten_models/ we have ten folders named run1, run2, up to run10, and each of those folder have at least the .res file generated by Terrier.
  - Run `python3 combine.py` to execute the first part of the project. This will read the 10 runs, normalize the scores and aggregate the run using the different fusion ranking techniques: combMNZ, combMAX, combMIN, combSUM, combANZ, combMED.
  - The resulting runs are saved in the output/ folder

  - If you want to test the plot.py script, then create a folder in input/ called "evaluations". We expect 16 or 17 trec_eval files (obtained with trec_eval library): one for each of the 10 single runs, 6 for the base strategies and, eventually, the last one, is the evaluation for ProbFuse.
  - Run 'python3 plot.py' to execute the simple plot script. It will create plots inside output/plots/.

  - If you want to test the eval.py script, then put the in input folder the "qrels.trec7.txt" file and the ten models we've shown above.
  - Run 'python3 evaluate.py' to pre-process our input files such that they'll be like: topic_id, doc_id, rel/not_rel (0/1).
  - This will be very useful to our ProbFuse.py script
