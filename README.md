# ProbFuse2006

This is the project for the Information Retrieval course at Unipd.
The work is split in two parts:
  - Reimplement the basic "comb" techniques following Fox and Shaw  "Combination of Multiple Searches" paper. We also normalize the scores following Lee "Combining Multiple Evidence from Different Properties of Weighting Schemes" (1995).
  - Implement an advanced fusion ranking technique, we choose the ProbFuse2006 paper, and study the reproducibility of the paper.

# How to use:
  - Use Python3
  - Create a folder in input/ called ten_models/.
       Inside this folder we expect the ten runs generated with Terrier of ten different IRS.
       So, in the folder ten_models/ there must be ten folders named run1, run2, ..., run10:
       each of those folders must have exactly one (no more, no less) '.res' file generated by Terrier
       (recall that Terrier generates three files, but only the '.res' file is needed).
  - Put the 'qrels.trec7.txt' file inside input/.
  - Put a propered COMPILED 'trec_eval9.0' folder in here, if you want to let our script run all the evaluations autonomously.
  - If you want, you can modify the input/Xtparams.txt file, to change the tuning parameters of ProbFuse.
       Leaving it as it is OK; if you want to modify it, please stick with the notation we've chose (e.g. X\t=\t[2, 3, ..., 6]).
  - If you want a complete run of our Project, just type `python3 RUNME.py` in a bash window inside the project folder:
       you will be guided in all the single steps of our project, which are:
         - Run the six base combination tecniques;
         - Run the pre-processing needed to make ProbFuse work properly
         - Run the ProbFuse approach with X segmentations and t% training queries;
         - Run the evaluations on everything done until this point (trec_eval);
         - Find the best parameter selection (X,t) for ProbFuse and print all the MAPS;
         - Plot the several MAP bar plots to get a nice comparison.
  - You can skip some of the steps above if you want to
       (this requires, obviously, that you've run all the steps in the correct order at least once).


# Development scripts

  - Run `python3 combine.py` to execute the first part of the project. This will read the 10 runs, normalize the scores and aggregate the run using
    the different fusion ranking techniques: combMNZ, combMAX, combMIN, combSUM, combANZ, combMED.
        - The resulting runs are saved in the output/ folder

  - If you want to test the plot.py script, then create a folder in input/ called "evaluations". We expect 16 or 17 trec_eval files (obtained with
    trec_eval library): one for each of the 10 single runs, 6 for the base strategies and, eventually, the last one, is the evaluation for ProbFuse.
        - Run 'python3 plot.py' to execute the simple plot script. It will create plots inside output/plots/.

  - If you want to test the eval.py script, then put the in input folder the "qrels.trec7.txt" file and the ten models we've shown above.
        - Run 'python3 evaluate.py' to pre-process our input files such that they'll be like: topic_id, doc_id, rel/not_rel (0/1).
        - This will be very useful to our ProbFuse.py script
